{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<B> First load all data using sklearn Api <B>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups as fetch_groups\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data = fetch_groups(subset ='all', categories= ['sci.med','comp.graphics'], shuffle=True, random_state=3116,return_X_y=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> defining the predictors and the target<b>\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Data[0]\n",
    "#print (data[1:100])\n",
    "target = Data[1]\n",
    "# remove stopping_words \n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 ... 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "print(target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<B> Here we extend the list of stop_words to include punc we use NLTK FOR THAT<b><br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1963, 32406)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "all_stopping_words = stopwords.words('english')\n",
    "punc = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~|.'''\n",
    "def split(punc): \n",
    "    return [char for char in punc]  \n",
    "    #adding punctuation\n",
    "punct = split(punc)     \n",
    "all_stopping_words.extend(punct)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\", '!', '(', ')', '-', '[', ']', '{', '}', ';', ':', \"'\", '\"', '\\\\', ',', '<', '>', '.', '/', '?', '@', '#', '$', '%', '^', '&', '*', '_', '~']\n"
     ]
    }
   ],
   "source": [
    "print (all_stopping_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Here we loop over all the documents we have and remove the stop words <b><br>\n",
    "1. we get every document then split if it contains a stop words we don't put it in the new text<br>\n",
    "2. you will find the a atest document, the first one before and after under the next cell the after is under'-----'<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: erikb@idt.unit.no (Erik Brenn)\n",
      "Subject: graphics formats\n",
      "Reply-To: erikb@idt.unit.no (Erik Brenn)\n",
      "Organization: Norwegian Institue of Technology\n",
      "Lines: 14\n",
      "\n",
      "I'm currently looking for information about different graphics\n",
      "formats, especially PPM, PCX BMP and perhaps GIF.\n",
      "Does anyone know if there exist any files at some site\n",
      "that describes these formats ???\n",
      "\n",
      "Thanks !\n",
      "\n",
      "\n",
      "-- \n",
      "          ~~~                       \n",
      "         (o o)           | Erik Brenn ,email: erikb@idt.unit.no\n",
      "        (  O  )   oOOO   | Faculty of Computer Science & Telematics\n",
      "         \\\\_//    / /    | The Norwegian Institute of Technology, Trondheim\n",
      "-oOOO--------------------| Not to make sense, just cents ! \n",
      "\n",
      "------\n",
      "erikb idt unit erik brenn subject graphics formats reply erikb idt unit erik brenn organization norwegian institue technology lines 14 currently looking information different graphics formats especially ppm pcx bmp perhaps gif anyone know exist files site describes formats thanks erik brenn email erikb idt unit oooo faculty computer science telematics norwegian institute technology trondheim oooo make sense cents \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "def remove_stopping_words(text,stop_words):\n",
    "    \n",
    "    new_text = ' '.join([word for word in re.split('\\W+', str(text).lower()) if word not in stop_words])\n",
    "    return new_text\n",
    "\n",
    "\n",
    "for i,document in enumerate(data):\n",
    "        \n",
    "        new_text= remove_stopping_words(document,all_stopping_words)\n",
    "        data[i] = new_text\n",
    "        if i ==0:\n",
    "            print(document)\n",
    "            print('------')\n",
    "            print(new_text)\n",
    "        \n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<B> Here we generate x_bag_of_words<B> <br>\n",
    "1. we get all the data and join them <br>\n",
    "2. we get all the unique data<br>\n",
    "3. then we define the size where number of rows are the number of documents and columns len of the unique words<br>\n",
    "4. each cell contains the number of occurences the column with index j appear in document i<br>\n",
    "5. i use the count method for that <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "one_text = ' '.join(data)\n",
    "one_text_splitted = one_text.split()\n",
    "dict_unique_words = set(one_text_splitted)\n",
    "X_bagWords = np.zeros([len(data),len(dict_unique_words)])\n",
    "#print(X_bagWords.shape)\n",
    "for i,document in enumerate(data) :\n",
    "    for j,value in enumerate(dict_unique_words):\n",
    "        if(value in document):\n",
    "              X_bagWords[i][j] = document.count(value)  \n",
    "                \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<B> Here we start to generate  x_tf_idf<B> <br>\n",
    "1. first we equate them with the bag_of words <br>\n",
    "2. then we get the indices where values not equal to zero<br>\n",
    "3. and for those values we divide the value by the corresponding length of the corresponding document<br>    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tf_idf = X_bagWords\n",
    "indices_not_zero = np.where(X_tf_idf != 0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is i 0 and this is j11\n"
     ]
    }
   ],
   "source": [
    "def generating_X_tf_idf():\n",
    "# putting the number of words in each document in the variable data_length\n",
    "    data_length = [ len(document.split()) for document in data ]\n",
    "# getting the i and j indices of the values not zero and dividing by the data length\n",
    "    for index in range(len(indices_not_zero)):\n",
    "        i = indices_not_zero[0][index]\n",
    "        j = indices_not_zero[1][index]\n",
    "        if(index == 0):\n",
    "            print(\"this is i {} and this is j{}\".format(i,j))\n",
    "        X_tf_idf[i][j] = (X_tf_idf[i][j])/(data_length[i])\n",
    "    return X_tf_idf\n",
    "\n",
    "    \n",
    "X_tf_idf = generating_X_tf_idf()        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<B> Here we start the process of splitting <B><br>\n",
    "1. first of all  we split the indices by the target value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#the target value represents the category_id so 0 --> 'sci.med' and 1 -->'comp.graphics' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# get the indices of every value \n",
    "indices_sci_med = np.where(target == 0 )[0] # holds the indices where category is sci_med\n",
    "indices_comp_graphics = np.where(target ==1)[0] # holds the indices where category is comp_graphics\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<B> here we make syre that it is strasfied_sampling<B> <br>\n",
    "1. we get the indices we got from above, 0.8 from the sci_med +0.8* comp_graphics go to training<br>\n",
    "2. same on test and validation but 0.1 each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(3116)\n",
    "train_indices_sci  = np.random.choice(indices_sci_med, int(0.8*len(indices_sci_med)),replace=False)\n",
    "train_indices_comp = np.random.choice(indices_comp_graphics, int(0.8*len(indices_comp_graphics)),replace=False)\n",
    "train_index  = np.concatenate((train_indices_sci,train_indices_comp))\n",
    "# Rest indecx\n",
    "Rest_indices_sci   = indices_sci_med[~np.isin(indices_sci_med,train_indices_sci)]\n",
    "Rest_indices_comp  = indices_comp_graphics[~np.isin(indices_comp_graphics ,train_indices_comp)]\n",
    "# vaidation index\n",
    "#print (len(Rest_indices_sci))\n",
    "validation_indices_sci  = np.random.choice(Rest_indices_sci, int(0.5*len(Rest_indices_sci)),replace=False)\n",
    "test_index_sci = Rest_indices_sci[~np.isin(Rest_indices_sci,validation_indices_sci)]\n",
    "validation_indices_comp = np.random.choice(Rest_indices_comp, int(0.5*len(Rest_indices_comp)),replace=False)\n",
    "test_index_comp = Rest_indices_comp[~np.isin(Rest_indices_comp,validation_indices_comp)]\n",
    "validation_index = np.concatenate((validation_indices_sci,validation_indices_comp))\n",
    "test_index = np.concatenate((test_index_sci,test_index_comp))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<B> The length of the indices for training and validation and testing is under teh next cell<B>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1570\n",
      "196\n",
      "197\n"
     ]
    }
   ],
   "source": [
    "print(len(train_index))\n",
    "print(len(validation_index))\n",
    "print (len(test_index))\n",
    "#print(target[train_index] ==1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<B>Exercise 1 - Implementing Naive Bayes Classifier<B>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<B>Sepearte by class method<B><br>\n",
    "1. based on the y_values it seperates the indices of the row of the dataframes <br>\n",
    "2. one with zero and one with one<br>\n",
    "3. it then return the dictionary of the indices of rows corresponding to every class<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seperate_by_class( y_values):\n",
    "    classes = {0:[],\n",
    "               1:[]}\n",
    "    for i in range(len(y_values)):\n",
    "        if y_values[i] == 0:\n",
    "           classes[0].append(i) \n",
    "        if y_values[i] == 1:\n",
    "           classes[1].append(i)\n",
    "    return classes        \n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<B>calculate_stastics_methods <B><br>\n",
    "1. this method is for calcuating the mean and std of every column  <br>\n",
    "2. it takes subset of dataframe bases on the indices passed<br>\n",
    "3. calcualtes mean and std for every column and return them<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def calculate_stats(x,indices):\n",
    "    stats={}\n",
    "    #print(indices)\n",
    "    x = pd.DataFrame(x)\n",
    "    subset_x = x.iloc[indices, :]\n",
    "    mean_not_f = subset_x.mean(axis = 0)\n",
    "    std_not_f = subset_x.std(axis = 0)\n",
    "    mean = []\n",
    "    std = [ ]\n",
    "    for i in range(len(mean_not_f)):\n",
    "       mean.append(mean_not_f[i])\n",
    "    for i in range (len(std_not_f)):\n",
    "        std.append(std_not_f[i])\n",
    "              \n",
    "    return [mean,std]\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> the naive_bayes_classfier method takes the predictors and the target <b><br>\n",
    "    1. it gets the indices corrsponding of every class by calling seperate_by_class<br>\n",
    "    2. then it goes and calcualte the statics for every column after they are seperated based on the class<br>\n",
    "    3. it returns that dictionary<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_bayes_classifer(x,y):\n",
    "    # first thing is to sperate by class\n",
    "    index_of_classes = seperate_by_class(y)\n",
    "    #print(index_of_classes)\n",
    "    # to calculate probablities of reaL_value we will use the gaussian_density function\n",
    "    statsics_by_class = {}\n",
    "    for key, values in index_of_classes.items() :\n",
    "        statsics_by_class[key] = calculate_stats(x,values)\n",
    "    return statsics_by_class    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> probablity_class method calculate probablity bu gaussian probality density function formula </b><br>\n",
    "1. it gets the mean and std of that specfic column we are calcuating the probablity on <br>\n",
    "2. and plug it in the equation <br>\n",
    "3.return thr probablity <b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def probablity_class(class_value, column_value, index_of_column,stats):\n",
    "    x = column_value\n",
    "    mean = stats[class_value][0][index_of_column]\n",
    "    std =  stats[class_value][1][index_of_column]\n",
    "    std_squared = std**2\n",
    "    # calculate gaussian probablity\n",
    "    second_variable = math.exp( (-1*(x-mean)**2)/(2*std_squared+0.1))  \n",
    "    first_variable = 1/(math.sqrt(2*math.pi*std_squared+0.1))\n",
    "    return first_variable * second_variable\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>predict_class method takes a row of the test_rows <b><br>\n",
    "1. calcultes the probality for zero and one and returns the class of the bigger one<br>\n",
    "2. the probality is accumaltive based on the conditional indeppendance we get the probality that class is\n",
    "zero and x1= something * pprobality that x2 =something and class 0 and so on till we finish the row<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_class(row,stats,p_zero,p_one):\n",
    "    #print(row)\n",
    "    probablity_zero =  p_zero\n",
    "    probablity_one = p_one\n",
    "    for i,column_value in enumerate(row):\n",
    "        probablity_zero *= probablity_class(0,column_value,i,stats) \n",
    "    #print(\"probality_zero is\", probablity_zero)\n",
    "    for j,column_value in enumerate(row):\n",
    "        probablity_one  *= probablity_class(1,column_value,j,stats)\n",
    "    #print(\"probality_one is\", probablity_one)\n",
    "    if(probablity_zero <= probablity_one ):\n",
    "        return 1\n",
    "    else:\n",
    "       return 0\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>The predict method<b><br>\n",
    "  1.calls predict_class on every row in the test dataset and return the presdictions and append it in an array<br>\n",
    "  2. return that array<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict (X,stats,p_zero,p_one):\n",
    "    predictions = []\n",
    "    for i,row in enumerate(X):\n",
    "        predictions.append(predict_class(row,stats,p_zero,p_one))\n",
    "    return predictions     \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> the test method was used to apply all the methods on small datsets and make sure that statiscs is corrct</b><br>\n",
    "1. the statsics were coreect u can see them under the next cell<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: [1, 2, 3], 1: [0, 4]}\n",
      "[1, 2, 3]\n",
      "[0, 4]\n",
      "the mean  of class[0] is :[1.3333333333333333, 4.666666666666667], the std of class[0] is :[0.5773502691896257, 2.081665999466133]\n",
      " the mean of class[1] is :[6.0, 10.5], the std of class[1] is:[4.242640687119285, 12.020815280171307]\n",
      "[3.393533211, 2.331273381]\n",
      "probality_zero is 5.464394352680922e-05\n",
      "probality_one is 0.0004102921642825742\n",
      "the predictions are : [1]\n"
     ]
    }
   ],
   "source": [
    "def test() :\n",
    "    x=[[3,2],[1,3],[2,4],[1,7],[9,19]]\n",
    "    y =[1,0,0,0,1]\n",
    "    x_test =[[3.393533211,2.331273381]]\n",
    "    stats = naive_bayes_classifer(x,y)\n",
    "    print(\"the mean  of class[0] is :{}, the std of class[0] is :{}\".format(stats[0][0],stats[0][1]))\n",
    "    print (\" the mean of class[1] is :{}, the std of class[1] is:{}\".format(stats[1][0],stats[1][1]))\n",
    "    zero = np.where(y == 0)\n",
    "    one = np.where (y==1)\n",
    "    p_zero = len(zero)/len(y) \n",
    "    p_one  = len(one)/len(y)\n",
    "    print(\"the predictions are :\",predict(x_test,stats,p_zero,p_one))\n",
    "#test()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Executing_naive_bayes_method </b>\n",
    "1. we get the predictions on the bag_of_words and the predictions on the tf-idf<br>\n",
    "2. we return them in an aray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "def executing_naive_bayes():\n",
    "    stats_bag_words = naive_bayes_classifer(X_bagWords[train_index],target[train_index])\n",
    "    zero = np.where(target[train_index]== 0)\n",
    "    one = np.where (target[train_index]== 1)\n",
    "    p_zero = len(zero)/len(target[train_index]) \n",
    "    p_one  = len(one)/len(target[train_index])\n",
    "    predictions_bag_words = predict(X_bagWords[test_index],stats_bag_words,  p_zero,p_one )\n",
    "    \n",
    "    #accuracy_bag_words = calc_acc(predictions_bag_words,target[test_index])\n",
    "    stats_tf_idf  = naive_bayes_classifer(X_tf_idf[train_index],target[train_index])\n",
    "    predictions_tf_idf = predict(X_tf_idf[test_index],stats_tf_idf,p_zero,p_one )\n",
    "    \n",
    "    #accuracy_tf_idf = calc_acc(predictions_tf_idf,target[test_index])\n",
    "    return [predictions_bag_words,predictions_tf_idf]\n",
    "\n",
    "predictions = executing_naive_bayes()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Here we calculate the accuracy of the predictions<b><br>\n",
    "1. once on the predictions of the bag_of_words<br>\n",
    "2. the other on tf-idf<br>\n",
    "2. the results are printed under the next cell<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The error percentage is bag_of_words is: 0.005076142131979695\n",
      "The error percentage is tf-idf is:  0.005076142131979695\n"
     ]
    }
   ],
   "source": [
    "def accuracy(predictions):\n",
    "    error =0\n",
    "    for i in range(len(target[test_index])):\n",
    "        if predictions[i] != target[test_index][i]:\n",
    "            error = error +1\n",
    "        return error/len(target[test_index])\n",
    "predictions_bag_of_words =  predictions[0]\n",
    "predictions_tf_idf =  predictions[1]\n",
    "error_percent_bag = accuracy(predictions_bag_of_words)\n",
    "print(\"The error percentage is bag_of_words is:\",error_percent_bag)\n",
    "error_percent_tf_idf = accuracy(predictions_tf_idf)\n",
    "print(\"The error percentage is tf-idf is: \",error_percent_tf_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<B>Exercise 2-  Implementing SVM Classifier via Scikit-Learn <B>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<B> The approach using the SVM classfier<B><br>\n",
    "1. executing_svm method takes a mode paramter where the mode can be bag_of_words or tf-idf<br>\n",
    "2. define paramter_space for grid_Search_o operate on<br>\n",
    "3. we take the best_paramters and pass them to the model<br>\n",
    "4. we fit in the training <br>\n",
    "5. predict using the predict method<br>\n",
    "6. calcualte teh accuracy using accuaracy_score<br>\n",
    "7. the method is then called once on the bag_of_words and once on tf-idf<br>\n",
    "8 the test_Scores are printed under the next cell<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "def executing_svm(mode):\n",
    "    paramter_space = {\n",
    "    'kernel':['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "     'C': [0.1,0.001,0.01,0.0001],\n",
    "     'gamma' :['scale', 'auto']    \n",
    "    }\n",
    "    grid = GridSearchCV(SVC(),param_grid = paramter_space)\n",
    "    svm_classfier = SVC()\n",
    "    if(mode == 'bag_of_words'):\n",
    "        grid.fit(X_bagWords[validation_index],target[validation_index])\n",
    "        svm_classfier.set_params(**grid.best_params_)\n",
    "        svm_classfier.fit(X_bagWords[train_index],target[train_index])\n",
    "        predictions = svm_classfier.predict(X_bagWords[test_index])\n",
    "        score = accuracy_score(target[test_index],predictions)\n",
    "        print(\"The test accuracy on bag_of_words : {}\".format(score))\n",
    "    elif(mode == 'tf-idf'):\n",
    "        grid.fit(X_bagWords[validation_index],target[validation_index])\n",
    "        svm_classfier.set_params(**grid.best_params_)\n",
    "        svm_classfier.fit(X_tf_idf[train_index],target[train_index])\n",
    "        predictions = svm_classfier.predict(X_tf_idf[test_index])\n",
    "        score = accuracy_score(target[test_index],predictions)   \n",
    "        print(\"The test accuracy on the tf-idf : {}\".format(score))\n",
    "    #print(\"The predictions are:{}\".format(predictions))\n",
    "    #print (\"The correct value are:{}\".format(target[test_index]))\n",
    "    \n",
    "        \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The test accuracy on bag_of_words : 0.949238578680203\n",
      "The test accuracy on the tf-idf : 0.949238578680203\n"
     ]
    }
   ],
   "source": [
    "def call_svm():\n",
    "    executing_svm('bag_of_words')\n",
    "    executing_svm('tf-idf')\n",
    "    \n",
    "call_svm()    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
